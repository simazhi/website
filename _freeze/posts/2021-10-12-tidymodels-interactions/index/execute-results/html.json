{
  "hash": "7cfaab3b99cdc7f8bc6dfdfba3406f13",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Tidymodels, interactions and anova - a short tutorial\ndate: '2021-10-12'\nslug: [tidymodels-anova]\ncategories: ['rstats', 'R']\nexecute:\n  freeze: auto\nimage: featured.png\n---\n\n\n\n![](featured.png)\n\nThe past month or so, I've become increasingly intrigued by the [tidymodels](https://tidymodels.org/) framework for doing modeling in R, especially after hearing an interview with Julia Silge on the [Not so standard deviations](https://nssdeviations.com/) podcast with Roger Peng and Hilary Parker.\n\nI envision myself writing more posts on this framework and applications to case studies from linguistics, but since this is the first post, I'll just share some thoughts I currently have regarding tidymodels.\n\n# Tidymodels: first thoughts\n\n- Just like with the [tidyverse](https://www.tidyverse.org/), I love the modularity and relatively clear sequence of steps that can guide you through \"any\" analysis. Although, we're not quite there yet in terms of straightforward applicability.\n- I love the tutorials and the \"tidytuesday\" series on YouTube. Those case studies really provide excellent showcases for how to do common things.\n- The idea of making the interfaces to different models more uniform so you can be more efficient.\n\nLet's talk metaphor for a second:\n\n- The conceptual metaphor of DATA IS MONEY, expressed in phrasings like *You need to spend your data budget wisely* are helpful. I generally don't see studies in linguistics that split the data first in a training and testing data set, although we probably should be doing that.\n- The conceptual metaphor of PREPPING DATA IS FOLLOWING A RECIPE (the [recipes](https://recipes.tidymodels.org/) package ) is highly amusing and makes sense.\n- The evocative names of the [other packages in the core tidymodels set](https://www.tidymodels.org/packages/) are good too: `rsample` for resampling, `parsnip` for modeling (this is a [parsnip](https://en.wikipedia.org/wiki/Parsnip), in Dutch it's called *pastinaak*), `workflows` for defining workflows, `tune` for tuning, `yardstick` for measing performance of your models, `broom` for cleaning the output of models...\n\nWhat I don't super like about tidymodels, is that as of yet, sometimes it's very hard to see what some tidymodels equivalents to super common modeling operations are. \n\n- For instance, I don't really know how to do an Multiple Correspondence Analysis (MCA) within tidymodels, even though the Principle Components Analysis (PCA) is very well represented in the tutorials and examples. I've tried some `recipes::step_dummy()` to get to similar results as I would have gotten with `FactoMineR` or `ca`, but I find the results not similar enough to surrender my dimension reductions for qualitative variables completely to tidymodels. **(Julia Silge and friends, if you are reading this, please figure this out for me.)**\n- I kind of hate that most analyses just stop at the \"Oh I collected the metrics, time to turn off my computer\" moment. I don't think the analysis is finished after calculating a model's performance; this interpretation is often lacking, and I don't know if that's a general quirk of data scientists or just specific to the showcases of these packages, but I wish it didn't stop there.\n- **Finally, we come to the topic of this post: how do I decide whether or not to keep interaction effects.** In \"normal\" modeling, this is quite straightforward:\n \n````\nStep 1. Make a model with multiple predictors, no interaction.\n\nStep 2. Make another model with the supposed interaction.\n\nStep 3. anova(model1, model2).\nIf the p value is significant, \nthe more complex model (the one with the interaction) is what you want.\n````\n\n# But what about the tidymodels approach to anovas?\n\nGoogling \"tidymodels anova\" will bring you to a lot of pages. \nUnfortunately, they are not super useful in answering our question:\n\n> RQ. How do I do a simple comparison of models to decide if the interaction is valid?\n\n[This page](https://broom.tidymodels.org/reference/tidy.anova.html) takes you to a `broom` function; [this one](https://www.tmwr.org/compare.html) to the excellent *Tidy modeling with R* book, more specifically, the chapter on comparing models with resampling.\nAnd I agree, resampling seems a very good way to compare models, and I've tried to apply that that page to the case study I will present below (nothing too crazy, just the `iris` dataset), but if I follow the guides there, I end up with no difference between the model with and without interactions, which is not what our anova will say. \nSo yeah. \nðŸ¤·ï¸\n\n# Let's load packages\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)   # general tidymodels packages\nlibrary(skimr)        # fancy way of inspecting data, not necessary\nlibrary(ggplot2)\n```\n:::\n\n\n\n# Let's look at the data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\niris %>% head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nskimr::skim(iris)\n```\n\n::: {.cell-output-display}\n\nTable: Data summary\n\n|                         |     |\n|:------------------------|:----|\n|Name                     |iris |\n|Number of rows           |150  |\n|Number of columns        |5    |\n|_______________________  |     |\n|Column type frequency:   |     |\n|factor                   |1    |\n|numeric                  |4    |\n|________________________ |     |\n|Group variables          |None |\n\n\n**Variable type: factor**\n\n|skim_variable | n_missing| complete_rate|ordered | n_unique|top_counts                |\n|:-------------|---------:|-------------:|:-------|--------:|:-------------------------|\n|Species       |         0|             1|FALSE   |        3|set: 50, ver: 50, vir: 50 |\n\n\n**Variable type: numeric**\n\n|skim_variable | n_missing| complete_rate| mean|   sd|  p0| p25|  p50| p75| p100|hist  |\n|:-------------|---------:|-------------:|----:|----:|---:|---:|----:|---:|----:|:-----|\n|Sepal.Length  |         0|             1| 5.84| 0.83| 4.3| 5.1| 5.80| 6.4|  7.9|â–†â–‡â–‡â–…â–‚ |\n|Sepal.Width   |         0|             1| 3.06| 0.44| 2.0| 2.8| 3.00| 3.3|  4.4|â–â–†â–‡â–‚â– |\n|Petal.Length  |         0|             1| 3.76| 1.77| 1.0| 1.6| 4.35| 5.1|  6.9|â–‡â–â–†â–‡â–‚ |\n|Petal.Width   |         0|             1| 1.20| 0.76| 0.1| 0.3| 1.30| 1.8|  2.5|â–‡â–â–‡â–…â–ƒ |\n\n\n:::\n:::\n\n\n\nWhat I will be investigating, is if Petal.Length can be predicted by Petal.Width and the Species.\nBut we want to know if there is an interaction between Petal.Width and Species.\nIn other words, our main model will look something like this:\n\n````\nPetal.Length ~ Petal.Width + (or *) Species\n````\n\nLet's plot the data:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\niris %>% \n  ggplot(aes(Petal.Width, Petal.Length)) +\n  geom_point(aes(color = Species)) +\n  geom_smooth(method = lm, color = \"orange\",\n              formula = 'y~x') + \n  geom_smooth(method = lm, aes(color = Species),\n              formula = 'y~x') +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\nWe can see three nice groups of Species. \nOur general linear model smooth (orange) doesn't seem too bad, but it doesn't take into account that there may be an interaction between the Species and the Petal.Width, as evidenced by the three different slopes per Species.\n\n# The \"normal\" way of testing this\n\nThe code will be quite short:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod1 <- lm(Petal.Length ~ Petal.Width * Species, data = iris)\n# summary(mod1)\n\nmod2 <- lm(Petal.Length ~ Petal.Width + Species, data = iris)\n# summary(mod2)\n\nanova(mod1, mod2) %>% tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 Ã— 7\n  term                          df.residual   rss    df sumsq statistic  p.value\n  <chr>                               <dbl> <dbl> <dbl> <dbl>     <dbl>    <dbl>\n1 Petal.Length ~ Petal.Width *â€¦         144  18.8    NA NA        NA    NA      \n2 Petal.Length ~ Petal.Width +â€¦         146  20.8    -2 -2.02      7.72  6.53e-4\n```\n\n\n:::\n:::\n\n\n\nWe see a very small p value (0.00065) so we know that there is a difference between `mod1` (no interaction) and `mod2` (with interaction), so we have to choose the one with interaction.\n\n**Done!**\nOr not?\n\n# Tidymodels\n\nTidymodels is a lot more verbose than just these three lines of code, but that's actually a good thing: **you can are more conscious of the steps involved**, **more explicit** and **can easily add more models**.\nHowever, here it will feel a bit redundant, but bear with me.\n\n## Data budget: `rsample`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\niris_split <- initial_split(iris, strata = Species, prop = 0.8)\niris_train <- training(iris_split)\niris_test  <- testing(iris_split)\n```\n:::\n\n\n\nThis is kind of the big difference compared to the normal modeling.\nWe split the data (and you can even make folds, but that's not for today) so that we can gauge the model's effectiveness later on, before reporting the model itself.\n\n## Preprocessing: `recipes`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# the model without interaction\nrec_normal <- \n  recipe(Petal.Length ~ Petal.Width + Species, \n         data = iris_train) %>%  # we train on the training set\n  step_dummy(all_nominal_predictors()) %>% \n  step_center(all_numeric_predictors())\n\n# the model with interaction\nrec_interaction <-\n  rec_normal %>% \n  step_interact(~ Petal.Width:starts_with(\"Species\"))\n```\n:::\n\n\n\nNotice that we manually create dummy variables (`step_dummy`) for all categorical predictors. In this case that's just Species. \nWe also center the numerical predictors because that's generally a good idea.\nFor the second model, we just have to add one extra step, that is declaring the interactions.\nYou can check the results of this recipe with the `prep()` function which can then be followed by the `bake(new_data = NULL)` function to see it in action.\n\n\n## Model selection: `parsnip`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\niris_model <-\n  linear_reg() %>% \n  set_engine(\"lm\") %>%     # if you want different engines, this is where you would do that\n  set_mode(\"regression\")\n```\n:::\n\n\n\n## Workflows: `workflows`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# normal workflow\niris_wf <-\n  workflow() %>% \n  add_model(iris_model) %>% \n  add_recipe(rec_normal)\n\n# interaction workflow\niris_wf_interaction <-\n  iris_wf %>% \n  update_recipe(rec_interaction)\n```\n:::\n\n\n\nOnce again, we can easily recycle workflows. \nIn workflows we bring together the recipe we made for preprocessing and the model we selected for the analysis.\nNote that we haven't run anything yet.\n\n## Fitting \n\nHere I'm making use of `last_fit()` on the split object. This makes sure the data is trained on the training dataset and evaluated on the test dataset. \nBut you can of course also `fit()` on the training or test set separately.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\niris_normal_lf <-\n  last_fit(iris_wf, \n           split = iris_split)\n\niris_inter_lf <-\n  last_fit(iris_wf_interaction, \n           split = iris_split)\n```\n:::\n\n\n\n## How to anova?\n\nThis is where I was stuck for the longest time.\nThe answer is actually surprisingly simple: we just use the normal `anova()` function, but we need to extract the linear model first.\nWe can do that with `extract_fit_engine()`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnormalmodel <- iris_normal_lf %>% extract_fit_engine()\nintermodel  <- iris_inter_lf %>% extract_fit_engine()\n\nanova(normalmodel, intermodel) %>% tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 Ã— 7\n  term                          df.residual   rss    df sumsq statistic  p.value\n  <chr>                               <dbl> <dbl> <dbl> <dbl>     <dbl>    <dbl>\n1 ..y ~ Petal.Width + Species_â€¦         116  17.8    NA NA        NA    NA      \n2 ..y ~ Petal.Width + Species_â€¦         114  16.1     2  1.72      6.10  0.00304\n```\n\n\n:::\n:::\n\n\n\nBam! Once again, p is significant.\n\n**But, why go through all this trouble?**\nKeep reading to get metrics and reasons.\n\n## Get metrics: `yardstick`\n\nNow that we know that the interaction model is the better one, we can also quickly get some metrics for that model.\nThe normal model is now irrelevant.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\niris_inter_lf %>% collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 Ã— 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard       0.318 Preprocessor1_Model1\n2 rsq     standard       0.968 Preprocessor1_Model1\n```\n\n\n:::\n:::\n\n\n\nCould we have found this with \"normal\" modeling?\nI guess so, but now we have also already tested it against \"new data\", i.e., the test data set we set aside in the beginning.\nSo we know the model **can predict reasonably well** and **does not overfit**.\nThere is **no data leakage**.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod2 %>% glance()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 Ã— 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.955         0.954 0.378     1036. 3.70e-98     3  -64.8  140.  155.\n# â„¹ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n```\n\n\n:::\n:::\n\n\n\n## What's in my model?\n\nThis is the step that most tutorials seem to neglect because it's not really in tidymodels; it's in general data analysis.\nBut what we usually report is not the root mean square deviation `rmse` (okay this just looks like a bunch of nouns strung together, but [see here](https://en.wikipedia.org/wiki/Root-mean-square_deviation)) or the R squared [RÂ²](https://en.wikipedia.org/wiki/Coefficient_of_determination) `rsq`, but the whole model.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nintermodel\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n                     (Intercept)                       Petal.Width  \n                          3.7894                            1.0309  \n              Species_versicolor                 Species_virginica  \n                          1.8314                            3.0006  \nPetal.Width_x_Species_versicolor   Petal.Width_x_Species_virginica  \n                          1.0536                           -0.2426  \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nintermodel %>% tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 Ã— 5\n  term                             estimate std.error statistic  p.value\n  <chr>                               <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)                         3.79      0.197    19.3   1.11e-37\n2 Petal.Width                         1.03      0.230     4.49  1.73e- 5\n3 Species_versicolor                  1.83      0.557     3.29  1.34e- 3\n4 Species_virginica                   3.00      0.585     5.13  1.22e- 6\n5 Petal.Width_x_Species_versicolor    1.05      0.652     1.62  1.09e- 1\n6 Petal.Width_x_Species_virginica    -0.243     0.621    -0.391 6.97e- 1\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nintermodel %>% glance()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 Ã— 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.958         0.956 0.375      516. 1.55e-76     5  -49.6  113.  133.\n# â„¹ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n```\n\n\n:::\n:::\n\n\n\nAnd finally, we can plot the model's predictions against the tested values.\nIf we draw an RÂ² plot we can see that it fits pretty well.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\niris_inter_lf %>% \n  collect_predictions() %>% \n  ggplot(aes(.pred, Petal.Length)) +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1, color = \"orange\") +\n  labs(x = \"Predicted Petal.Length\",\n       y = \"Observed Petal.Length\",\n       title = \"RÂ² plot\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n\nSo, tidymodels is definitely a longer style of analysis but you can get much more out of your data. \nAnd isn't that what we ultimately want?\nWe have:\n\n- shown that indeed the interaction is there: the petal width can predict the petal length, but there is an influence of the species\n- made a model that is protected against overfitting\n- tested said model against some of the data in the model (so we know it's more robust)\n- observed the fit of the model (RÂ²)\n\nOf course, these are \"just plants\" (sorry biased, love languages and I couldn't get my mint to sprout so may still be vengeful about that).\nBut now there's a tutorial on how to do simple anova within a tidymodels framework for deciding if you should keep an interaction or not.\n\n# Disclaimer on the iris data set\n\nIn recent years, it has become more public knowledge that the ubiquitous `iris` data set was first published in the *Annals of Eugenics* in 1936 by [Ronald Fisher](https://en.wikipedia.org/wiki/Ronald_Fisher#Eugenics). \nAs [this tweet](https://twitter.com/kareem_carr/status/1271096239103369224?lang=en) and [this post](https://armchairecology.blog/iris-dataset/) point out, it's perhaps not the best thing that this data set is so readily used in data examples.\nSome proposals for other data sets can be found [here](https://www.meganstodel.com/posts/no-to-iris/). \n\nObviously, **eugenics is bad** -- think of how important this issue was to the projected future in Star Trek with Khan and friends -- and I agree that `iris` is kind of boring. \nBut, just like the *Annals of Eugenics* rebranded itself to the [*Annals of Human Genetics*](https://en.wikipedia.org/wiki/Annals_of_Human_Genetics), distancing themselves from that terrible phase in science (and we still see the beast rear its head once in a while), I can't help but think that this silly description of flowers is quite innocent.\nPerhaps, death of the scholar does exist?\nAfter all, if we have to throw away `iris` because of Fisher's bad personal views (once again, not good), do we also have to throw out the stats techniques he developed? \nI know that I've used the Fisher-Yates Exact Test to calculate mutual attraction.\n\nAnd let's also talk about [Karl Pearson](https://en.wikipedia.org/wiki/Karl_Pearson), who was the first editor of the *Annals of Eugenics*. \nReading up on his wiki bio was not pleasant either.\nSo should we throw out the Pearson correlation?\nOr even worse: the p-value (which was first *formally* introduced in the Pearson's chi-square test)?\nGone with Principled Components Analysis!\nHistograms? History you mean!\n\nThe point is that it is necessary to treat the data and work as separate from their personal life.\nThat means that I agree with the efforts to [rename buildings that were named after Fisher](https://en.wikipedia.org/wiki/Ronald_Fisher#Reappraisal_of_his_contentious_views_on_race_and_eugenics) or [Pearson](https://en.wikipedia.org/wiki/Karl_Pearson#Politics_and_eugenics) at UCL, but that at the same time we should still be okay with using `iris` or statistical techniques developed by these people. \nThe best two arguments for not using `iris` are that it's boring and that there exists a `penguins` dataset ([found here](https://allisonhorst.github.io/palmerpenguins/)).\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}