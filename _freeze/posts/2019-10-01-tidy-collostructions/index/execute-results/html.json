{
  "hash": "e6ad53d3213923e89a2bbb89fed43a81",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Tidy collostructions\"\ndate: '2019-10-01'\ncategories: ['rstats', 'R', 'academic workflow', 'coding', 'association measures']\nexecute:\n  freeze: auto\nimage: collostructions.png\n---\n\n\n\n\n\n![](collostructions.png)\n\n\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n# tl ; dr\n\nIn this post I look at the family of [collexeme analysis](https://en.wikipedia.org/wiki/Collostructional_analysis) methods originated by Gries and Stefanowitsch. \nSince they use a lot of Base R, and love using vectors, there is a hurdle that needs to be conquered if you are used to the rectangles in tidy data.\nI first give an overview of what the method tries to do, and then at the end show the hurdle in detail, followed by the steps necessary to enable the computation of the desired variables and statistical tests (association measures). \nBasically, you need to convert from pure tidy data to a tidy contingency format.\n\n# Introduction\n\nSo in order to finalize my database on Chinese ideophones, creatively entitled [CHIDEOD](https://osf.io/kpwgf/), I decided to work through [Stefan Gries](http://www.stgries.info)'s *Quantitative corpus linguistics with R* (2016; 2nd edition; [companion website here](http://www.stgries.info/research/qclwr/qclwr.html)).  \nThat together with [Natalia Levshina](http://www.natalialevshina.com)'s *How to do linguistics with R* (2015; 1st edition; [companion website here](https://benjamins.com/sites/z.195/)), which I worked through last June, has given me a lot of inspiration to tackle a number issues I have been struggling with, or at least thinking about without really knowing how to tackle them — I guess that counts as *struggling with*.\nBy the way, it is locally kind of confirmed currently that Levshina will be a keynote speaker at our CLDC conference in May.\nSo you can already prepare those abstracts if you wish to attempt to present here in Taipei.\n\nAnyway, one of the more intriguing family of methods to investigate the relation between constructions and words that fill the slots is the family of collexeme analysis.\n\n# What is collexeme analysis?\n\nAs [Gries (2019)](https://doi.org/10.1075/ijcl.00011.gri) discusses, the methodology of collexeme analysis is an extension of the notions of a) *collocation* (which words really belong together, e.g. *watch TV* and *watch a movie* will have a stronger collocational bond than *watch a powerpoint presentation*, although the latter has become a weekly activity), and b) *colligation* (what constructions belong together, e.g. *watch* will typically be followed by a noun, although verb phrases like *watch him play* also occur).\n\nSo the basis of this method is a [contingency table](https://en.wikipedia.org/wiki/Contingency_table) (or cross-table; or [*kruistabel*](https://nl.wikipedia.org/wiki/Kruistabel) if you love Dutch; *lièlián biǎo* 列聯表 in Chinese (apparently)).\nBasically, something like this, that should look somewhat familiar:\n\n|                                  | Element 2 | Not element 2 / other elements | Sum\n|----------------------------------|-----------|--------------------------------|---\n**Element 1 **                     | a         |        b                       | a + b\n**Not element 1 / other elements** | c         |     d                          | c + d\n**Sum**                            | a + c     | b + d                  | a + b + c + d\n\nThese letters (a, b, c, d) play an important role in colloxeme analyses.\nSo stay tuned for that.\n\n## Approach 1: Collexeme analysis\nSo what [Stefanowitsch & Gries (2003)](https://doi.org/10.1075/ijcl.8.2.03ste) wanted to investigate was the [N *waiting to happen*] construction, e.g. *an accident waiting to happen*, *a disaster waiting to happen* etc. \nIn this **collexeme analysis** you are quantifying to what degree the words found in a corpus occur in that construction: are they attracted or repulsed, and by how much?\n\nIn this scenario, element 1 is e.g. *an accident*, and element 2 the construction *waiting to happen*.\n\n|                                  | *waiting to happen* | not *waiting to happen* | Sum\n|----------------------------------|-----------|--------------------------------|---\n***an accident***                     | a         |        b                       | a + b\nnot ***an accident*** | c         |     d                          | c + d\n**Sum**                            | a + c     | b + d                  | a + b + c + d\n\n\n## Approach 2: Distinctive collexeme analysis\n\nA year later, [Gries & Stefanowitsch (2004)](https://doi.org/10.1075/ijcl.9.1.06gri) extended the methodology to quantify to what degree the words prefer to appear in one of two constructions. \nFor example, the ditransitive *give him a call* vs. the prepositional *give a call to him*. \nHere the table changes into:\n\n|                                  | ditransitive | prepositional | Sum\n|----------------------------------|-----------|--------------------------------|---\n***give***                     | a         |        b                       | a + b\nnot ***give*** | c         |     d                          | c + d\n**Sum**                            | a + c     | b + d                  | a + b + c + d\n\n## Approach 3: Co-varying collexeme analysis\n\nThe third seminal paper was published in the same year (Gries & Stefanowitsch 2004; ISBN: 9781575864648) sought to quantify the attraction / repulsion between two different slots in a construction, e.g. *trick ... into buying*, *force ... into accepting* etc. \nFor this, the table is adapted to:\n\n|                                  | *accepting* | not *accepting* | Sum\n|----------------------------------|-----------|--------------------------------|---\n***force***                     | a         |        b                       | a + b\nnot ***force*** | c         |     d                          | c + d\n**Sum**                            | a + c     | b + d                  | a + b + c + d\n\n\n## Okay, I get that table stuff, what next?\n\nLet's say you were able to get the frequencies from a corpus -- I should probably mention that, `*cough* *cough*` not everybody agrees with this kind of contingency tables, the overview paper by [Gries (2019)](https://doi.org/10.1075/ijcl.00011.gri) has plenty of interesting references and exciting rebuttals -- and you have this table, possibly for lots of elements and/or constructions.\n\nWe'll keep an example here that I calculated with Gries's (2016) book mentioned above.\nOne of the case studies looks at verbs that co-occur with the modal verb *must*, e.g. *must accept*, *must agree*, *must confess*, vs. how much these verbs occur with other modal verbs, e.g *should accept*, *should agree* etc.\nSo in this case, element 1 was a verb (*confess*) and element 2 was *must*; not-element-1 were all the other verbs, and not-element-2 were all the other modal verbs.\n\nThe respective table from an abstract level looks like this:\n\n|                                  | *must* | not *must* | Sum\n|----------------------------------|-----------|--------------------------------|---\n***confess***                     | a         |        b                       | a + b\nnot ***confess*** | c         |     d                          | c + d\n**Sum**                            | a + c     | b + d                  | a + b + c + d\n\nI got these frequencies from the BNC (as Gries shows in his book, but I used my tidyverse skills^TM^ to get them, so if they are slightly off, then it was because of not following his script completely):\n\n|                                  | *must* | not *must* | Sum\n|----------------------------------|-----------|--------------------------------|---\n***confess***                     | 11         |        1                       | 12\nnot ***confess*** | 1990         |     62114                          | 64104\n**Sum**                            | 2001     | 62115                  | 64116\n\nOr in a Base R table:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\na <- 11\nb <- 1\nc <- 1990\nd <- 62114\n\nconfess <- c(a, b)\nnotconfess <- c(c, d)\n\nmust.table <- rbind(confess, notconfess) \ncolnames(must.table) <- c(\"must\", \"notmust\")\nmust.table\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           must notmust\nconfess      11       1\nnotconfess 1990   62114\n```\n\n\n:::\n:::\n\n\n\n\n\n\nWhat do you do now?\n\n*Now it's time for letter math!*\n\nThe association measure (read: statistical test) that Gries & Stefanowitsch, as well as others, have used most is the [Fisher Yates Exact test](https://en.wikipedia.org/wiki/Fisher%27s_exact_test), and more precisely the negative $log10$ of its $p$-value.\nUnderlying this (see the link in this paragraph) are calculations using those letter cells (a,b,c,d).\nLuckily we don't need to do that manually because R has a function for this --  `fischer.test()`:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfisher.test(must.table)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tFisher's Exact Test for Count Data\n\ndata:  must.table\np-value = 3.106e-16\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n    49.90447 13241.49567\nsample estimates:\nodds ratio \n  344.6593 \n```\n\n\n:::\n:::\n\n\n\n\n\nSo as you can see the pvalue is 3.1056007\\times 10^{-16}. \nIf we take the negative $log10$ of this, it becomes 15.5078544, which gives us the result we expected.\n\nAn easier way of computing the Fisher Yates Exact test of this table is by using the letter math and the `pv.Fischer.collostr` function provided by Levshina's `Rling` package:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nRling::pv.Fisher.collostr(a, b, c, d)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3.105601e-16\n```\n\n\n:::\n:::\n\n\n\n\n\n\nOther tests that have been proposed are so-called Reliance and Attraction (cf. [Schmid & Küchenhoff 2013](doi.org/10.1515/cog-2013-0018) a.o.).\nReliance is the relative frequency of a verb (*confess*) with *must* with regard to all uses of the given verb; Attraction is the relative frequency of a verb (*confess*) with *must* based on all usages with *must*.\n\n$$ Reliance = \\frac{a}{a+b}$$\n\n$$ Attraction = \\frac{a}{a+c} $$\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nattraction <- a / (a+c) * 100\nreliance <- a / (a+b) * 100\n```\n:::\n\n\n\n\n\nThe Attraction of confess and must is 0.5497251 and the Reliance is 91.6666667.\nThis high Reliance means that whenever *confess* occurs in the corpus after a modal it *relies* on *must* to occur.\nIts Attraction, however, has a much lower value: *must* does not necessarily occur with *confess*, in fact, it occurs with lots of other verbs as well!\n\nThe third group of tests is actually correlated to Attraction and Reliance: respectively  $\\Delta P_{word \\to construction}$ and $\\Delta P_{construction \\to word}$ (cf. [Ellis & Ferreira-Junior](doi.org/10.1075/arcl.7.08ell) a.o.).\nThey are also known as tests of cue validity and are calculated as follows:\n\n$$\\Delta P_{word \\to construction} = cue_{construction} = \\frac{a}{a + c} - \\frac{b}{b + d}$$\n\n$$\\Delta P_{construction \\to word} = cue_{verb} = \\frac{a}{a + b} - \\frac{c}{c + d}$$\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndP.cueCx <- a/(a + c) - b/(b + d)\ndP.cueVerb <- a/(a + b) - c/(c + d)\n```\n:::\n\n\n\n\n\nSo the $\\Delta P_{word \\to construction}$ of *must confess* is 0.0054812\nand the $\\Delta P_{construction \\to word}$ of *must confess* is 0.8856234, so these numbers look a lot like those of Attraction and Reliance.\n\nAnyway, I think you get the drift: if you have the contingency table, you choose an association measure (there are more than these three sets) and analyze the results.\n\n# Tidy collostructions\n\nSo if that's so clear, why am I writing this post?\nFor me, the main difficulty with the approach is that Gries and Levshina love writing things in Base R (Gries even more than Levshina).\nBut to someone who really started to appreciate R after the tidyverse became more available, there is this dissonance with the way they go about things.\n\nOne of the biggest difference is the obsessive-compulsion of tidyverse to think in rectangles, a.k.a. dataframes or tibbles, rather than the vector( letter)s that Gries and Levshina love using, especially in their letter mathematics.\n\nAs a consequence of this \"[rectangling](https://www.youtube.com/watch?v=GapSskrtUzU)\" and tidy format (long and skinny), rather than contingency format (2 x 2), it is challenging to compute even basic chisquare.tests.\n\nSo in Base R, a chisquare is easy to compute, but then hard to continue working with, because this thick text block is given and you can't really access the contents (but there are solutions, like `broom::tidy`):\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchisq.test(must.table)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in chisq.test(must.table): Chi-squared approximation may be incorrect\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's Chi-squared test with Yates' continuity correction\n\ndata:  must.table\nX-squared = 282.63, df = 1, p-value < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n\n\nIn a tidy format, you typically have to jump through a lot of hoops to either get to use the same function or use one of the newer alternative functions:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nmust.tibble <- tribble(\n  ~ verb, ~ must, ~notmust,\n  \"confess\", 11, 1,\n  \"notconfess\", 1990, 62114\n)\nmust.tibble %>% kable()\n```\n\n::: {.cell-output-display}\n\n\n|verb       | must| notmust|\n|:----------|----:|-------:|\n|confess    |   11|       1|\n|notconfess | 1990|   62114|\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# this function will give you bad output\n# so you can't just simply do this\nchisq.test(must.tibble$must, must.tibble$notmust)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in chisq.test(must.tibble$must, must.tibble$notmust): Chi-squared\napproximation may be incorrect\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's Chi-squared test with Yates' continuity correction\n\ndata:  must.tibble$must and must.tibble$notmust\nX-squared = 0, df = 1, p-value = 1\n```\n\n\n:::\n:::\n\n\n\n\n\n## Hoop 1: make it really tidy\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmust.tibble %>%\n  pivot_longer(cols = c(must, notmust),\n               names_to = \"modal\",\n               values_to = \"n\") %>% # now it is really tidy\n  kable()\n```\n\n::: {.cell-output-display}\n\n\n|verb       |modal   |     n|\n|:----------|:-------|-----:|\n|confess    |must    |    11|\n|confess    |notmust |     1|\n|notconfess |must    |  1990|\n|notconfess |notmust | 62114|\n\n\n:::\n:::\n\n\n\n\n\n## Hoop 2: uncount\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmust.tibble %>%\n  pivot_longer(cols = c(must, notmust),\n               names_to = \"modal\",\n               values_to = \"n\") %>% # now it is really tidy\n  uncount(n)  # uncount\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 64,116 × 2\n   verb    modal\n   <chr>   <chr>\n 1 confess must \n 2 confess must \n 3 confess must \n 4 confess must \n 5 confess must \n 6 confess must \n 7 confess must \n 8 confess must \n 9 confess must \n10 confess must \n# ℹ 64,106 more rows\n```\n\n\n:::\n:::\n\n\n\n\n\n## Hoop 3: convert to table\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmust.tibble %>%\n  pivot_longer(cols = c(must, notmust),\n               names_to = \"modal\",\n               values_to = \"n\") %>% # now it is really tidy\n  uncount(n)  %>% # uncount\n  table() # turn to Base R table\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            modal\nverb          must notmust\n  confess       11       1\n  notconfess  1990   62114\n```\n\n\n:::\n:::\n\n\n\n\n\n## Hoop 4: chisquare\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmust.tibble %>%\n  pivot_longer(cols = c(must, notmust),\n               names_to = \"modal\",\n               values_to = \"n\") %>% # now it is really tidy\n  uncount(n)  %>% # uncount\n  table() %>% # turn to Base R table\n  chisq.test() \n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in chisq.test(.): Chi-squared approximation may be incorrect\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's Chi-squared test with Yates' continuity correction\n\ndata:  .\nX-squared = 282.63, df = 1, p-value < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n\n\n## Hoop 5: `tidy` with `broom`\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmust.tibble %>%\n  pivot_longer(cols = c(must, notmust),\n               names_to = \"modal\",\n               values_to = \"n\") %>% # now it is really tidy\n  uncount(n)  %>% # uncount\n  table() %>% # turn to Base R table\n  chisq.test()%>%\n  broom::tidy() %>%\n  kable()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in chisq.test(.): Chi-squared approximation may be incorrect\n```\n\n\n:::\n\n::: {.cell-output-display}\n\n\n| statistic| p.value| parameter|method                                                       |\n|---------:|-------:|---------:|:------------------------------------------------------------|\n|  282.6321|       0|         1|Pearson's Chi-squared test with Yates' continuity correction |\n\n\n:::\n:::\n\n\n\n\n\nI know there are some tidyverse-friendly functions like `infer::chisq_test`, but it seems to lack arguments like expected values (`chisq.test()$exp`). \nSo this awkward hoop jumping is annoying but gets the job done (for now?).\n\n## From tidy to contingency\n\nAnyway, while for most summary statistics a tidy format (see hoop 1) is the easiest to work with, I don't think it's very intuitive for the association measures paradigm.\n\nSo in this working example of *must + V_inf_* construction what I did was get all the occurrences of all modal verbs + verbs in the infinitive from the BNC corpus.\nThe second step I did was add a column with `dplyr::case_when` to identify if a modal verb was *must* or another modal verb.\n\nLet's look at this data shall we:\n\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndf.must\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 64,116 × 3\n   modal  verb      mod.type\n   <chr>  <chr>     <chr>   \n 1 can    find      OTHER   \n 2 should stop      OTHER   \n 3 should recognise OTHER   \n 4 will   cost      OTHER   \n 5 might  think     OTHER   \n 6 can    sink      OTHER   \n 7 can    change    OTHER   \n 8 must   help      MUST    \n 9 ll     help      OTHER   \n10 ll     take      OTHER   \n# ℹ 64,106 more rows\n```\n\n\n:::\n:::\n\n\n\n\n\nAs you can see there are 64116 rows, and the table is just a count away form being tidy.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy.df.must <- df.must %>%\n  count(mod.type, verb, sort = TRUE)\ntidy.df.must\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2,522 × 3\n   mod.type verb      n\n   <chr>    <chr> <int>\n 1 OTHER    get    3750\n 2 OTHER    go     3421\n 3 OTHER    see    2409\n 4 OTHER    take   2056\n 5 OTHER    like   1778\n 6 OTHER    say    1647\n 7 OTHER    come   1460\n 8 OTHER    make   1383\n 9 OTHER    give   1245\n10 OTHER    put    1162\n# ℹ 2,512 more rows\n```\n\n\n:::\n:::\n\n\n\n\n\nMuch 'prettier', but what next?\nThis is actually the mental step I struggled the most with, this dissonance between tidy and contingency. \nHowever, since we coded `mod.type` with a binary value: \"OTHER\" or \"MUST\", it is actually trivial to `dplyr::spread` or `dplyr::pivot_wider` them to a \"tidy contingency table\":\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspread.tidy.df.must <- tidy.df.must %>%\n  pivot_wider(values_from = n,\n              names_from = mod.type) \nspread.tidy.df.must\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2,113 × 3\n   verb  OTHER  MUST\n   <chr> <int> <int>\n 1 get    3750   102\n 2 go     3421    94\n 3 see    2409    14\n 4 take   2056    63\n 5 like   1778     3\n 6 say    1647   105\n 7 come   1460    41\n 8 make   1383    62\n 9 give   1245    22\n10 put    1162    18\n# ℹ 2,103 more rows\n```\n\n\n:::\n:::\n\n\n\n\n\nHey, this looks a lot like those schematic tables we had at the beginning!\nBut now the real challenge is to turn these numbers into the letters a, b, c, and d, so we can perform our letter math.\n\nAfter changing all numeric values we have to doubles (instead of integers) and changing all NAs to 0, we can get the letters by following simple arithmetic from our original schematic table, e.g. if we know a and a+c, then c = a+c - a etc.\n\n|                                  | Element 2 | Not element 2 / other elements | Sum\n|----------------------------------|-----------|--------------------------------|---\n**Element 1 **                     | a         |        b                       | a + b\n**Not element 1 / other elements** | c         |     d                          | c + d\n**Sum**                            | a + c     | b + d                  | a + b + c + d\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspread.tidy.df.must %>%\n  mutate_if(is.numeric, ~ as.double(.x)) %>% # we'll want doubles instead of integers\n  mutate_if(is.numeric, ~ replace_na(.x, 0)) %>% # NAs should be 0\n  \n  mutate(a = MUST,\n       ac = sum(MUST),\n       c = ac - a,\n       ab = MUST + OTHER,\n       b = ab - a,\n       abcd = sum(MUST, OTHER),\n       d = abcd - a - b -c,\n       aExp = (a + b)*(a + c)/(a + b + c + d)) -> abcd.must\nabcd.must\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2,113 × 11\n   verb  OTHER  MUST     a    ac     c    ab     b  abcd     d  aExp\n   <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1 get    3750   102   102  2001  1899  3852  3750 64116 58365 120. \n 2 go     3421    94    94  2001  1907  3515  3421 64116 58694 110. \n 3 see    2409    14    14  2001  1987  2423  2409 64116 59706  75.6\n 4 take   2056    63    63  2001  1938  2119  2056 64116 60059  66.1\n 5 like   1778     3     3  2001  1998  1781  1778 64116 60337  55.6\n 6 say    1647   105   105  2001  1896  1752  1647 64116 60468  54.7\n 7 come   1460    41    41  2001  1960  1501  1460 64116 60655  46.8\n 8 make   1383    62    62  2001  1939  1445  1383 64116 60732  45.1\n 9 give   1245    22    22  2001  1979  1267  1245 64116 60870  39.5\n10 put    1162    18    18  2001  1983  1180  1162 64116 60953  36.8\n# ℹ 2,103 more rows\n```\n\n\n:::\n:::\n\n\n\n\n\nSo now we can easily perform all of the association measures we want, and rank them accordingly.\n\n## Fischer Yates Exact\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfye <- abcd.must %>%\n  mutate(fye = Rling::pv.Fisher.collostr(a, b, c, d)) %>%\n  #filter(verb == \"confess\") %>%\n  mutate(negfye = case_when(a < aExp ~ format(round(log10(fye)), nsmall = 2),\n                            TRUE ~ format(round(- log10(fye)), nsmall = 2)),\n         negfye = as.double(negfye)) %>% #I did some rounding\n  arrange(desc(negfye)) %>%\n  select(verb, OTHER, MUST, fye, negfye)\n\nfye %>% top_n(20) %>% kable()\n```\n\n::: {.cell-output-display}\n\n\n|verb       | OTHER| MUST|       fye| negfye|\n|:----------|-----:|----:|---------:|------:|\n|admit      |    11|  159| 0.0000000|    225|\n|confess    |     1|   11| 0.0000000|     16|\n|say        |  1647|  105| 0.0000000|      9|\n|realise    |    22|   11| 0.0000000|      9|\n|recognise  |    46|   13| 0.0000000|      8|\n|know       |   432|   38| 0.0000001|      7|\n|remember   |   324|   33| 0.0000000|      7|\n|rank       |     3|    6| 0.0000001|      7|\n|decide     |   120|   17| 0.0000014|      6|\n|pay        |   354|   30| 0.0000052|      5|\n|wait       |   139|   16| 0.0000307|      5|\n|ensure     |    53|   10| 0.0000243|      5|\n|inform     |     5|    5| 0.0000065|      5|\n|stress     |     5|    5| 0.0000065|      5|\n|apologise  |     1|    3| 0.0001186|      4|\n|obey       |     2|    3| 0.0002895|      4|\n|look       |   560|   33| 0.0017845|      3|\n|feel       |   223|   18| 0.0006318|      3|\n|act        |    47|    7| 0.0014017|      3|\n|assume     |    24|    5| 0.0018726|      3|\n|reflect    |    24|    5| 0.0018726|      3|\n|mention    |    23|    5| 0.0015902|      3|\n|emerge     |    13|    4| 0.0016252|      3|\n|own        |     9|    4| 0.0005395|      3|\n|emphasise  |     7|    3| 0.0030892|      3|\n|register   |     7|    3| 0.0030892|      3|\n|balance    |     6|    3| 0.0022137|      3|\n|realize    |     6|    3| 0.0022137|      3|\n|respect    |     3|    3| 0.0005656|      3|\n|comply     |     1|    2| 0.0028599|      3|\n|exhibit    |     1|    2| 0.0028599|      3|\n|export     |     0|    2| 0.0009735|      3|\n|outperform |     0|    2| 0.0009735|      3|\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfye %>% arrange(negfye) %>% top_n(-20) %>% kable()\n```\n\n::: {.cell-output-display}\n\n\n|verb    | OTHER| MUST|       fye| negfye|\n|:-------|-----:|----:|---------:|------:|\n|like    |  1778|    3| 0.0000000|    -20|\n|see     |  2409|   14| 0.0000000|    -18|\n|hear    |   428|    0| 0.0000021|     -6|\n|help    |   820|    7| 0.0000185|     -5|\n|want    |   401|    0| 0.0000074|     -5|\n|tell    |   985|   13| 0.0003130|     -4|\n|give    |  1245|   22| 0.0031496|     -3|\n|put     |  1162|   18| 0.0006828|     -3|\n|need    |   461|    4| 0.0018961|     -3|\n|lead    |   321|    1| 0.0009838|     -3|\n|imagine |   290|    1| 0.0019299|     -3|\n|call    |   268|    0| 0.0003216|     -3|\n|cause   |   211|    0| 0.0022792|     -3|\n|buy     |   402|    4| 0.0090712|     -2|\n|play    |   340|    4| 0.0291563|     -2|\n|happen  |   335|    2| 0.0039267|     -2|\n|afford  |   205|    0| 0.0035887|     -2|\n|receive |   205|    1| 0.0242516|     -2|\n|pick    |   203|    1| 0.0239273|     -2|\n|appear  |   187|    0| 0.0049951|     -2|\n|benefit |   147|    0| 0.0160195|     -2|\n\n\n:::\n:::\n\n\n\n\n\n\n## Attraction, Reliance\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nabcd.must %>%\n  mutate(attraction = a / (a+c) * 100,\n         reliance = a / (a+b) * 100) %>%\n  arrange(desc(attraction)) %>%\n  top_n(20) %>%\n  select(verb, OTHER, MUST, attraction, reliance) %>%\n  kable()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nSelecting by reliance\n```\n\n\n:::\n\n::: {.cell-output-display}\n\n\n|verb        | OTHER| MUST| attraction| reliance|\n|:-----------|-----:|----:|----------:|--------:|\n|export      |     0|    2|   0.099950|      100|\n|outperform  |     0|    2|   0.099950|      100|\n|acquaint    |     0|    1|   0.049975|      100|\n|capitalise  |     0|    1|   0.049975|      100|\n|capitulate  |     0|    1|   0.049975|      100|\n|class       |     0|    1|   0.049975|      100|\n|combat      |     0|    1|   0.049975|      100|\n|comfort     |     0|    1|   0.049975|      100|\n|discard     |     0|    1|   0.049975|      100|\n|dissociate  |     0|    1|   0.049975|      100|\n|dread       |     0|    1|   0.049975|      100|\n|enclose     |     0|    1|   0.049975|      100|\n|graft       |     0|    1|   0.049975|      100|\n|interrogate |     0|    1|   0.049975|      100|\n|itch        |     0|    1|   0.049975|      100|\n|nurture     |     0|    1|   0.049975|      100|\n|plagiarise  |     0|    1|   0.049975|      100|\n|protrude    |     0|    1|   0.049975|      100|\n|rediscover  |     0|    1|   0.049975|      100|\n|redouble    |     0|    1|   0.049975|      100|\n|shoulder    |     0|    1|   0.049975|      100|\n|shrive      |     0|    1|   0.049975|      100|\n|stipulate   |     0|    1|   0.049975|      100|\n\n\n:::\n:::\n\n\n\n\n\n## Delta P\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nabcd.must %>%\n  mutate(dP.cueCx = a/(a + c) - b/(b + d),\n         dP.cueVerb = a/(a + b) - c/(c + d)) %>%\n  arrange(desc(dP.cueCx)) %>%\n  select(verb,  OTHER, MUST, dP.cueCx, dP.cueVerb) %>%\n  top_n(20) %>%\n  kable()\n```\n\n::: {.cell-output-display}\n\n\n|verb        | OTHER| MUST|  dP.cueCx| dP.cueVerb|\n|:-----------|-----:|----:|---------:|----------:|\n|export      |     0|    2| 0.0009995|  0.9688212|\n|outperform  |     0|    2| 0.0009995|  0.9688212|\n|acquaint    |     0|    1| 0.0004998|  0.9688061|\n|capitalise  |     0|    1| 0.0004998|  0.9688061|\n|capitulate  |     0|    1| 0.0004998|  0.9688061|\n|class       |     0|    1| 0.0004998|  0.9688061|\n|combat      |     0|    1| 0.0004998|  0.9688061|\n|comfort     |     0|    1| 0.0004998|  0.9688061|\n|discard     |     0|    1| 0.0004998|  0.9688061|\n|dissociate  |     0|    1| 0.0004998|  0.9688061|\n|dread       |     0|    1| 0.0004998|  0.9688061|\n|enclose     |     0|    1| 0.0004998|  0.9688061|\n|graft       |     0|    1| 0.0004998|  0.9688061|\n|interrogate |     0|    1| 0.0004998|  0.9688061|\n|itch        |     0|    1| 0.0004998|  0.9688061|\n|nurture     |     0|    1| 0.0004998|  0.9688061|\n|plagiarise  |     0|    1| 0.0004998|  0.9688061|\n|protrude    |     0|    1| 0.0004998|  0.9688061|\n|rediscover  |     0|    1| 0.0004998|  0.9688061|\n|redouble    |     0|    1| 0.0004998|  0.9688061|\n|shoulder    |     0|    1| 0.0004998|  0.9688061|\n|shrive      |     0|    1| 0.0004998|  0.9688061|\n|stipulate   |     0|    1| 0.0004998|  0.9688061|\n\n\n:::\n:::\n\n\n\n\n\n# Conclusion\n\nIt is perfectly possible to perform association measures starting with a tidy dataframe.\nFirst you need to spread out with a binary variable, to make it a tidy contingency table.\nThen you can identify a, b, c, and d.\nNext you can chooose your preferred statistical test, rank verbs, and try to interpret the findings.\n\nI should probably also mention that Gries is has been advocating to not just use one association measure, but three or more.\nUsing just two, you can plot like Attraction and Reliance.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nabcd.must %>%\n  mutate(attraction = a / (a+c) * 100,\n         reliance = a / (a+b) * 100) %>%\n  arrange(desc(attraction)) %>%\n  select(verb, OTHER, MUST, attraction, reliance) %>%\n  ggplot(aes(x = attraction, \n             y = reliance)) +\n   geom_point() +\n  gghighlight::gghighlight(attraction > 2 | 50 < reliance & reliance < 100,\n                           label_key = verb) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}